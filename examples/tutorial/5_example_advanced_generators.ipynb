{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom `Generator` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example should guide you to build your own simple generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from adaptivemd import (\n",
    "    Project, Task, File, PythonTask\n",
    ")\n",
    "\n",
    "project = Project('tutorial')\n",
    "\n",
    "engine = project.generators['openmm']\n",
    "modeller = project.generators['pyemma']\n",
    "pdb_file = project.files['initial_pdb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have completed at least some of the previous examples and have a general idea of how adaptiveMD works. Still, let's recapitulate what we think is the typical way of a simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to execute something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute something you need\n",
    "\n",
    "1. a description of the task to be done. This is the `Task` object. Once you have this you can,\n",
    "2. use it in a `Scheduler` which will interpret the `Task` into some code that the computer understands. It handles all the little things you expect from the task, like registering generated file, etc... And to do so, the `Scheduler` needs\n",
    "3. your `Resource` description which acts like a config for the scheduler\n",
    "\n",
    "When you have a `Scheduler` (with `Resource`) you let it execute `Task` objects. If you know how to build these you are done. That is all you need. \n",
    "\n",
    "### What are `Generator`s?\n",
    "\n",
    "Build a task can be cumbersome and often repetative, and a factory for `Task` objects is extremely useful. These are called `Generator`s (maybe `TaskFactory`) is a better name?!?\n",
    "\n",
    "In your final scheme where you observe all generated objects and want to build new tasks accordingly you will (almost) never build a `Task` yourself. You use a generator. \n",
    "\n",
    "A typical example is an `Engine`. It will generate tasks, that simulate new trajectories, extend existing ones, etc... Basic stuff. The second big class is `Analysis`. It will use trajectories to generate models or properties of interest to guide your decisions for new trajectories. \n",
    "\n",
    "In this example we will build a simple generator for a task, that uses the `mdtraj` package to compute some features and store these in the database and in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `MDTrajFeaturizer` generator\n",
    "\n",
    "First, we think about how this featurizer works if we would not use `adaptivemd`. The reason is, that we have basically two choices for designing a `Task` (see example 4 about `Task` objects). \n",
    "\n",
    "1. A task that calls bash commands for you\n",
    "2. A task that calls a python function for you\n",
    "\n",
    "Since we want to call `mdtraj` functions we use the 2nd and start with a skeleton for this type and store it under `my_generator.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% file my_generator.py\n",
    "# This is an example for building your own generator\n",
    "# This file must be added to the project so that it is loaded\n",
    "# when you import `adaptivemd`. Otherwise your worker don't know\n",
    "# about the class!\n",
    "\n",
    "from adaptivemd import Generator\n",
    "\n",
    "class MDTrajFeaturizer(Generator):\n",
    "    def __init__(self, {things we always need}):\n",
    "        super(PyEMMAAnalysis, self).__init__()\n",
    "\n",
    "        # stage file you want to reuse (optional)\n",
    "        # self['pdb_file'] = pdb_file\n",
    "        # stage = pdb_file.transfer('staging:///')\n",
    "        # self['pdb_file_stage'] = stage.target\n",
    "        # self.initial_staging.append(stage)\n",
    "\n",
    "    @staticmethod\n",
    "    def then_func(project, task, data, inputs):\n",
    "        # add the output for later reference\n",
    "        project.data.add(data)\n",
    "\n",
    "    def execute(self, {options per task}):\n",
    "\n",
    "        t = PythonTask(self)\n",
    "\n",
    "        # get your staged files (optional)\n",
    "        # input_pdb = t.link(self['pdb_file_stage'], 'input.pdb')\n",
    "        \n",
    "        # add the python function call to your script (there can be only one!)\n",
    "        t.call(\n",
    "            my_script,\n",
    "            param1,\n",
    "            param2,\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        return t\n",
    "        \n",
    "def my_script(param1, param2, ...):\n",
    "    return {\"whatever you want to return\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What input does our generator always need?\n",
    "\n",
    "Mdtraj needs a topology unless it is already present. Interestingly, our `Trajectory` objects know about their topology so we could access these, if our function is to process a `Trajectory`. This requires the `Trajectory` to be the input. If we want to process any file, then we might need a topology. \n",
    "\n",
    "The decision if we want the generator to work for a fixed topology is yours. To show how this would work, we do this here. We use a fixed topology per generator that applies to `File` objects.\n",
    "\n",
    "Second is the feature we want to compute. This is tricky and so we hard code this now. You can think of a better way to represent this. But let's pick the tertiary stucture prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, pdb_file=None):\n",
    "    super(PyEMMAAnalysis, self).__init__()\n",
    "\n",
    "    # if we provide a pdb_file it should be used\n",
    "    if pdb_file is not None:\n",
    "        # stage file you want to reuse (optional)\n",
    "\n",
    "        # give the file an internal name\n",
    "        self['pdb_file'] = pdb_file\n",
    "        # create the transfer from local to staging:\n",
    "        stage = pdb_file.transfer('staging:///')\n",
    "        # give the staged file an internal name\n",
    "        self['pdb_file_stage'] = stage.target\n",
    "        # append the transfer action to the initial staging action list\n",
    "        self.initial_staging.append(stage)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The task building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute(self, file_to_analyze):\n",
    "    \n",
    "    assert(isinstance(file_to_analyze, File))\n",
    "\n",
    "    t = PythonTask(self)\n",
    "\n",
    "    # get your staged files (optional)\n",
    "    if self.get('pdb_file_stage'):\n",
    "        input_pdb = t.link(self['pdb_file_stage'], 'input.pdb')\n",
    "    else:\n",
    "        input_pdb = None\n",
    "\n",
    "    # add the python function call to your script (there can be only one!)\n",
    "    t.call(\n",
    "        my_script,\n",
    "        file_to_analyze,\n",
    "        input_pdb\n",
    "    )\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The actual script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is executed on the HPC for you. And requires mdtraj to be installed on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_script(file_to_analyze, input_pdb):\n",
    "    import mdtraj as md\n",
    "    \n",
    "    traj = md.load(file_to_analyze, top=input_pdb)\n",
    "    features = traj.compute_xyz()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. At least in the simplest form. When you use this to create a `Task` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_generator = MDTrajFeaturizer(pdb_file)\n",
    "task = my_generator.execute(traj.file('master.dcd'))\n",
    "project.queue(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wait and then the `Task` object has a `.output` property which now contains the returned result. \n",
    "\n",
    "This can now be used in your execution plans..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy():\n",
    "    # generate some structures...\n",
    "    # yield wait ...\n",
    "    # get a traj object\n",
    "    task = my_generator.execute(traj.outputs('master'))\n",
    "    # wait until the task is done\n",
    "    yield task.is_done\n",
    "    # print the output\n",
    "    output = task.output\n",
    "    # do something with the result, store in the DB, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better storing of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you want to save the output from your function in the DB in some form or another. Though the output is stored, it is not conviniently accessed unless you know the task that was used. \n",
    "\n",
    "For this reason there is a callback function you can set, that can take care of doing a custom handling of the output. The function to be called needs to be a method of the generator and you can give the task the name of the method. The name (str) of the funtion can be set using the `then()` command. An the default name is `then_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute(self, ...):\n",
    "    t = PythonTask(self)\n",
    "    t.then('handle_my_output')\n",
    "    \n",
    "@staticmethod\n",
    "def handle_my_output(project, task, data, inputs):\n",
    "    print 'Saving data from task', task, 'into model'\n",
    "    m = Model(data)\n",
    "    project.model.add(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes exactly 4 parameters \n",
    "\n",
    "1. `project`: the project in which the task was run. Is used to access the database, etc\n",
    "2. `task`: the actual task object that produced the output\n",
    "3. `data`: the output returned by the function\n",
    "4. `inputs`: the input to the python function call (internally). The data actually transmitted to the worker to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the above example you can do whatever you want with your data, store it, alter it, write it to a file, etc. In case you do not want to additionally save the output (`data`) in the DB as an object, you can tell the trask not to by setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute(self, ...):\n",
    "    t = PythonTask(self)\n",
    "    t.then('handle_my_output')\n",
    "    t.store_output = False  # default is `True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in that case `.output` will stay `None` even after execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with `Trajectory` files and get their properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you always have to write file generation and file analysis/reading that matches. We only store some very general properties of objects with them, e.g. a stride for trajectories. This means you cannot arbitrarily mix code for these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want that this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_generator.execute(traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is rather simple: All you need to do is to extract the actual files from the trajectory object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __init__(self, outtype, pdb_file=None):\n",
    "    super(PyEMMAAnalysis, self).__init__()\n",
    "    \n",
    "    # we store a str that holds the name of the outputtype\n",
    "    # this must match the definition \n",
    "    self.outtype = outtype\n",
    "\n",
    "    # ...\n",
    "\n",
    "def execute(self, traj, *args, **kwargs):\n",
    "    t = PythonTask(self)\n",
    "    # ...\n",
    "    file_location = traj.outputs(self.outtype)  # get the trajectory file matching outtype\n",
    "    # use the file_location.\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import!** You have no access to the `Trajectory` object in our remove function. These will be converted\n",
    "to a real path relative to the working directory. This makes sure that you will not have to deal with\n",
    "prefixes, etc. This might change in the future, but. The scripts are considered independent of adaptivemd!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with saving your generator to the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not complicated but you need to briefly learn about the mechanism to store complex Python objects in the DB. The general way to Store an instance of a class requires you to subclass from `adaptivemd.mongodb.StorableMixin`. This provides the class with a `__uuid__` attribute that is a unique number for each storable object that is given at creation time. (If we would just store objects using pymongo we would get a number like this, but later). Secondly, it add two functions\n",
    "\n",
    "1. `to_dict()`: this converts the (immutable) state of the object into a dictionary that is simple enough that it can be stored. Simple enought means, that you can have Python primitives, things like numpy arrays or even other storable objects, but not arbitrary objects in it, like lambda constructs (these are possible but need special treatment)\n",
    "2. `from_dict()`: The reverse. It takes the dictionary from `to_dict` and must return an equivalent object!\n",
    "\n",
    "So, you can do\n",
    "\n",
    "```\n",
    "clone = obj.__class__.from_dict(obj.to_dict())\n",
    "```\n",
    "\n",
    "and get an equal object in that it has the same attributes. You could also say a deep copy.\n",
    "\n",
    "This is not always trivial and there exists a default implementation, which will make an additional assumption:\n",
    "\n",
    "All necessary attributes have the same parameters in `__init__`. So, this would correspond to this rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyStorableObject(StorableMixin):\n",
    "    def __init__(self, state):\n",
    "        self.state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while this would not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyStorableObject(StorableMixin):\n",
    "    def __init__(self, initial_state):\n",
    "        self.state = initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second case you need to overwrite the default function. All of these will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix `to_dict` to match default `from_dict`\n",
    "class MyStorableObject(StorableMixin):\n",
    "    def __init__(self, initial_state):\n",
    "        self.state = initial_state\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'initial_state': self.state \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix `from_dict` to match default `to_dict`\n",
    "class MyStorableObject(StorableMixin):\n",
    "    def __init__(self, initial_state):\n",
    "        self.state = initial_state\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dict(cls, dct):\n",
    "        return cls(initial_state=dct['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix both `from_dict` and `to_dict`\n",
    "class MyStorableObject(StorableMixin):\n",
    "    def __init__(self, initial_state):\n",
    "        self.state = initial_state\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'my_state': self.state \n",
    "        }\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dict(cls, dct):\n",
    "        return cls(initial_state=dct['my_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do that, make sure that you really capture all variables. Especially if you subclass from an existing one. You can use super to access the result from the parent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyStorableObject(StorableMixin):\n",
    "    @classmethod\n",
    "    def from_dict(cls, dct):\n",
    "        obj = super(MyStorableObject, cls).from_dict(dct)\n",
    "        obj.missing_attr1 = dct['missing_attr_key1']\n",
    "        return obj\n",
    "\n",
    "    def to_dict(self):\n",
    "        dct = super(MyStorableObject, self).to_dict(self)\n",
    "        dct.update({\n",
    "            'missing_attr_key1': self.missing_attr1\n",
    "        })\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the recommended way to build your custom functions. For completeness we show here what the base `TaskGenerator` class will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@classmethod\n",
    "def from_dict(cls, dct):\n",
    "    obj = cls.__new__(cls)\n",
    "    StorableMixin.__init__(obj)\n",
    "    obj._items = dct['_items']\n",
    "    obj.initial_staging = dct['initial_staging']\n",
    "    return obj\n",
    "\n",
    "def to_dict(self):\n",
    "    return {\n",
    "        '_items': self._items,\n",
    "        'initial_staging': self.initial_staging\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only unfamiliar part is the\n",
    "\n",
    "```py\n",
    "obj = cls.__new__(cls)\n",
    "StorableMixin.__init__(obj)\n",
    "```\n",
    "\n",
    "which needs a little explanation. \n",
    "\n",
    "In most `__init__` functions for a `TaskGenerator` you will construct the `initial_staging` attribute with some functions. If you would reconstruct by just calling the constructor with the same parameters again, this would result in an equal object as expected and that would work, but not in all regards as expected: The problem is that if you generate objects that can be stored, these will get new UUIDs and hence are considered different from the ones that you wanted to store. In short, the construction in the `__init__` prevents you from getting the real old object back, you always construct something new.\n",
    "\n",
    "This can be solved by not using `__init__` but creating an empty object using `__new__` and then fixing all attributes to the original state. This is very similar to `__setstate__` which we do not use in general to still allow using `__init__` which makes sense in most cases where not storable objects are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we discuss an existing generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word about this example. While a `Task` can be created and configured a new class in `adaptivemd` needs to be part of the project. So we will write discuss the essential parts of the existing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator is in essence a factory to create `Task` objects with a single command. A generator can be initialized with certain files that the created tasks will always need, like an engine will need a topology for each task, etc. It also (as explained briefly before in Example 4) knows about certain callback behaviour of their tasks. Last, a generator allows you to assign a worker only to tasks that were created by a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the code of the PyEMMAAnalysis\n",
    "\n",
    "```py\n",
    "class PyEMMAAnalysis(Analysis):\n",
    "    def __init__(self, pdb_file):\n",
    "        super(PyEMMAAnalysis, self).__init__()\n",
    "\n",
    "        self['pdb_file'] = pdb_file\n",
    "        stage = pdb_file.transfer('staging:///')\n",
    "\n",
    "        self['pdb_file_stage'] = stage.target\n",
    "        self.initial_staging.append(stage)\n",
    "\n",
    "    @staticmethod\n",
    "    def then_func(project, task, model, inputs):\n",
    "        # add the input arguments for later reference\n",
    "        model.data['input']['trajectories'] = inputs['files']\n",
    "        model.data['input']['pdb'] = inputs['topfile']\n",
    "        project.models.add(model)\n",
    "\n",
    "    def execute(\n",
    "            self,\n",
    "            trajectories,\n",
    "            tica_lag=2,\n",
    "            tica_dim=2,\n",
    "            msm_states=5,\n",
    "            msm_lag=2,\n",
    "            stride=1):\n",
    "\n",
    "        t = PythonTask(self)\n",
    "\n",
    "        input_pdb = t.link(self['pdb_file_stage'], 'input.pdb')\n",
    "        t.call(\n",
    "            remote_analysis,\n",
    "            trajectories=list(trajectories),\n",
    "            topfile=input_pdb,\n",
    "            tica_lag=tica_lag,\n",
    "            tica_dim=tica_dim,\n",
    "            msm_states=msm_states,\n",
    "            msm_lag=msm_lag,\n",
    "            stride=stride\n",
    "        )\n",
    "\n",
    "        return t\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def __init__(self, pdb_file):\n",
    "    # don't forget to call super\n",
    "    super(PyEMMAAnalysis, self).__init__()  \n",
    "\n",
    "    # a generator also acts like a dictionary for files\n",
    "    # this way you can later access certain files you might need\n",
    "    \n",
    "    # save the pdb_file under the same name\n",
    "    self['pdb_file'] = pdb_file  \n",
    "\n",
    "    # this creates a transfer action like it is used in tasks\n",
    "    # and moves the passed pdb_file (usually on the local machein)\n",
    "    # to the staging_area root directory\n",
    "    stage = pdb_file.transfer('staging:///')\n",
    "    \n",
    "    # and the new target file (which is also like the original) \n",
    "    # on the staging_area is saved unter `pdb_file_stage`\n",
    "    # so, we can access both files if we wanted to\n",
    "    # note that the original file most likely is in the DB\n",
    "    # so we could just skip the stage transfer completely\n",
    "    self['pdb_file_stage'] = stage.target\n",
    "    \n",
    "    # last we add this transfer to the initial_staging which\n",
    "    # is done only once per used generator\n",
    "    self.initial_staging.append(stage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# the kwargs is to keep the exmaple short, you should use explicit\n",
    "# parameters and add appropriate docs\n",
    "def execute(self, trajectories, **kwargs):\n",
    "    # create the task and set the generator to self, our new generator\n",
    "    t = PythonTask(self)\n",
    "\n",
    "    # we want to copy the staged file to the worker directory\n",
    "    # and name it `input.pdb`\n",
    "    input_pdb = t.link(self['pdb_file_stage'], 'input.pdb')\n",
    "    \n",
    "    # if you chose not to use the staging file and copy it directly you\n",
    "    # would use in analogy\n",
    "    # input_pdb = t.link(self['pdb_file'], 'input.pdb')\n",
    "\n",
    "    # finally we use `.call` and want to call the `remote_analysis` function\n",
    "    # which we imported earlier from somewhere\n",
    "    t.call(\n",
    "        remote_analysis,\n",
    "        trajectories=list(trajectories),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a call_back function. The name `then_func` is the default function name to be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# we use a static method, but you can of course write a normal method\n",
    "@staticmethod\n",
    "# the call_backs take these arguments in this order\n",
    "# the second parameter is actually a `Model` object in this case\n",
    "# which has a `.data` attribute\n",
    "def then_func(project, task, model, inputs):\n",
    "    # add the input arguments for later reference to the model\n",
    "    model.data['input']['trajectories'] = inputs['kwargs']['files']\n",
    "    model.data['input']['pdb'] = inputs['kwargs']['topfile']\n",
    "    # and save the model in the project\n",
    "    project.models.add(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief summary and things you need to set to make your generator work\n",
    "\n",
    "```py\n",
    "class MyGenerator(Analysis):\n",
    "    def __init__(self, {things your generator always needs}):\n",
    "        super(MyGenerator, self).__init__()\n",
    "        \n",
    "        # Add input files to self\n",
    "        self['file1'] = file1\n",
    "\n",
    "        # stage all files to the staging area of you want to keep these\n",
    "        # files on the HPC\n",
    "        for fn in ['file1', 'file2', ...]:\n",
    "            stage = self[fn].transfer('staging:///')\n",
    "            self[fn + '_stage'] = stage.target\n",
    "            self.initial_staging.append(stage)\n",
    "\n",
    "    @staticmethod\n",
    "    def then_func(project, task, outputs, inputs):\n",
    "        # do something with input and outputs\n",
    "        # store something in your project\n",
    "\n",
    "    def task_using_python_rpc(\n",
    "            self,\n",
    "            {arguments}):\n",
    "\n",
    "        t = PythonTask(self)\n",
    "\n",
    "        # set any task dependencies if you need\n",
    "        t.dependencies = []\n",
    "                \n",
    "        input1 = t.link(self['file1'], 'alternative_name1')\n",
    "        input2 = t.link(self['file2'], 'alternative_name2')\n",
    "        ...\n",
    "\n",
    "        # add whatever bash stuff you need BEFORE the function call\n",
    "        t.append('some bash command')\n",
    "        ...\n",
    "\n",
    "        # use input1, etc in your function call if you like. It will\n",
    "        # be converted to a regular file location you can use\n",
    "        t.call(\n",
    "            {my_remote_python_function},\n",
    "            files=list(files),\n",
    "        )\n",
    "\n",
    "        # add whatever bash stuff you need AFTER the function call\n",
    "        t.append('some bash command')\n",
    "        ...\n",
    "\n",
    "        return t\n",
    "\n",
    "    def task_using_bash_argument_call(\n",
    "            self,\n",
    "            {arguments}):\n",
    "\n",
    "        t = Task(self)\n",
    "\n",
    "        # set any task dependencies if you need\n",
    "        t.dependencies = []\n",
    "\n",
    "        input1 = t.link(self['file1'], 'alternative_name1')\n",
    "        input2 = t.link(self['file2'], 'alternative_name2')\n",
    "        ...\n",
    "        # add more staging\n",
    "        t.append({action})\n",
    "        ...\n",
    "\n",
    "        # add whatever bash stuff you want to do\n",
    "        t.append('some bash command')\n",
    "        ...\n",
    "\n",
    "        # add whatever staging stuff you need AFTER the function call\n",
    "        t.append({action})\n",
    "        ...\n",
    "        \n",
    "        return t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplified code for the OpenMMEngine\n",
    "\n",
    "```py\n",
    "class OpenMMEngine(Engine):\n",
    "    trajectory_ext = 'dcd'\n",
    "\n",
    "    def __init__(self, system_file, integrator_file, pdb_file, args=None):\n",
    "        super(OpenMMEngine, self).__init__()\n",
    "\n",
    "        self['pdb_file'] = pdb_file\n",
    "        self['system_file'] = system_file\n",
    "        self['integrator_file'] = integrator_file\n",
    "        self['_executable_file'] = exec_file\n",
    "\n",
    "        for fn in self.files:\n",
    "            stage = self[fn].transfer(Location('staging:///'))\n",
    "            self[name + '_stage'] = stage.target\n",
    "            self.initial_staging.append(stage)\n",
    "\n",
    "        if args is None:\n",
    "            args = '-p CPU --store-interval 1'\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "    # this one only works if you start from a file\n",
    "    def task_run_trajectory_from_file(self, target):\n",
    "        # we create a special Task, that has some additional functionality\n",
    "        t = TrajectoryGenerationTask(self, target)\n",
    "\n",
    "        # link all the files we require\n",
    "        initial_pdb = t.link(self['pdb_file_stage'], Location('initial.pdb'))\n",
    "        t.link(self['system_file_stage'])\n",
    "        t.link(self['integrator_file_stage'])\n",
    "        t.link(self['_executable_file_stage'])\n",
    "\n",
    "        # use the initial PDB to be used\n",
    "        input_pdb = t.get(target.frame, 'coordinates.pdb')\n",
    "\n",
    "        # this represents our output trajectory\n",
    "        output = Trajectory('traj/', target.frame, length=target.length, engine=self)\n",
    "\n",
    "        # create the directory so openmmrun can write to it\n",
    "        t.touch(output)\n",
    "\n",
    "        # build the actual bash command\n",
    "        cmd = 'python openmmrun.py {args} -t {pdb} --length {length} {output}'.format(\n",
    "            pdb=input_pdb,\n",
    "            length=target.length,\n",
    "            output=output,\n",
    "            args=self.args,\n",
    "        )\n",
    "        t.append(cmd)\n",
    "        \n",
    "        # copy the resulting trajectory directory back to the staging area\n",
    "        t.put(output, target)\n",
    "\n",
    "        return t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
