{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaptiveMD\n",
    "\n",
    "## Example 4 - Custom `Task` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from adaptivemd import (\n",
    "    Project, Task, File, PythonTask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open our `test` project by its name. If you completed the first examples this should all work out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project = Project('tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open all connections to the `MongoDB` and `Session` so we can get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see again where we are. These numbers will depend on whether you run this notebook for the first time or just continue again. Unless you delete your project it will accumulate models and files over time, as is our ultimate goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StoredBundle for with 222 file(s) @ 0x10b9ada10>\n",
      "<StoredBundle for with 2 file(s) @ 0x10b9ad9d0>\n",
      "<StoredBundle for with 34 file(s) @ 0x10b9ad990>\n"
     ]
    }
   ],
   "source": [
    "print project.files\n",
    "print project.generators\n",
    "print project.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now restore our old ways to generate tasks by loading the previously used generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = project.generators['openmm']\n",
    "modeller = project.generators['pyemma']\n",
    "pdb_file = project.files['initial_pdb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task is in essence a bash script-like description of what should be executed by the worker. It has details about files to be linked to the working directory, bash commands to be executed and some meta information about what should happen in case we succeed or fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first explain briefly how a task is executed and what its components are. This was originally build so that it is compatible with radical.pilot and still is. So, if you are familiar with it, all of the following information should sould very familiar.\n",
    "\n",
    "A task is executed from within a unique directory that only exists for this particular task. These are located in `adaptivemd/workers/` and look like \n",
    "\n",
    "```\n",
    "worker.0x5dcccd05097611e7829b000000000072L/\n",
    "```\n",
    "\n",
    "the long number is a hex representation of the UUID of the task. Just if you are curious type\n",
    "```\n",
    "print hex(my_task.__uuid__)\n",
    "```\n",
    "\n",
    "Then we change directory to this folder write a `running.sh` bash script and execute it. This script is created from the task definition and also depends on your resource setting (which basically only contain the path to the workers directory, etc)\n",
    "\n",
    "The script is divided into 1 or 3 parts depending on which `Task` class you use. The main `Task` uses a single list of commands, while `PrePostTask` has the following structure\n",
    "\n",
    "1. **Pre-Exec**: Things to happen before the main command (optional)\n",
    "\n",
    "2. **Main**: the main commands are executed\n",
    "\n",
    "3. **Post-Exec**: Things to happen after the main command (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, lots of theory, now some real code for running a task that generated a trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task = engine.task_run_trajectory(project.new_trajectory(pdb_file, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Link('staging:///alanine.pdb' > 'worker://initial.pdb),\n",
       " Link('staging:///system.xml' > 'worker://system.xml),\n",
       " Link('staging:///integrator.xml' > 'worker://integrator.xml),\n",
       " Link('staging:///openmmrun.py' > 'worker://openmmrun.py),\n",
       " Touch('worker://traj/'),\n",
       " 'python openmmrun.py -r --report-interval 1 -p CPU --store-interval 1 -t worker://initial.pdb --length 100 worker://traj/',\n",
       " Move('worker://traj/' > 'sandbox:///{}/00000076/)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are linking a lot of files to the worker directory and change the name for the .pdb in the process. Then call the actual `python` script that runs openmm. And finally move the `output.dcd` and the restart file back tp the trajectory folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to list lot's of things about tasks and we will use it a lot to see our modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: TrajectoryGenerationTask(OpenMMEngine) [created]\n",
      "\n",
      "Sources\n",
      "- staging:///integrator.xml \n",
      "- staging:///alanine.pdb \n",
      "- staging:///openmmrun.py \n",
      "- staging:///system.xml \n",
      "Targets\n",
      "- sandbox:///{}/00000076/\n",
      "Modified\n",
      "\n",
      "<pretask>\n",
      "Link('staging:///alanine.pdb' > 'worker://initial.pdb)\n",
      "Link('staging:///system.xml' > 'worker://system.xml)\n",
      "Link('staging:///integrator.xml' > 'worker://integrator.xml)\n",
      "Link('staging:///openmmrun.py' > 'worker://openmmrun.py)\n",
      "Touch('worker://traj/')\n",
      "python openmmrun.py -r --report-interval 1 -p CPU --store-interval 1 -t worker://initial.pdb --length 100 worker://traj/\n",
      "Move('worker://traj/' > 'sandbox:///{}/00000076/)\n",
      "<posttask>\n"
     ]
    }
   ],
   "source": [
    "print task.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as a task is not saved and hence placed in the queue, it can be altered in any way. All of the 3 / 5 phases can be changed separately. You can add things to the staging phases or bash phases or change the command. So, let's do that now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a bash line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a `Task` is very similar to a list of bash commands and you can simply append (or prepend) a command. A text line will be interpreted as a bash command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task.append('echo \"This new line is pointless\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: TrajectoryGenerationTask(OpenMMEngine) [created]\n",
      "\n",
      "Sources\n",
      "- staging:///integrator.xml \n",
      "- staging:///alanine.pdb \n",
      "- staging:///openmmrun.py \n",
      "- staging:///system.xml \n",
      "Targets\n",
      "- sandbox:///{}/00000076/\n",
      "Modified\n",
      "\n",
      "<pretask>\n",
      "Link('staging:///alanine.pdb' > 'worker://initial.pdb)\n",
      "Link('staging:///system.xml' > 'worker://system.xml)\n",
      "Link('staging:///integrator.xml' > 'worker://integrator.xml)\n",
      "Link('staging:///openmmrun.py' > 'worker://openmmrun.py)\n",
      "Touch('worker://traj/')\n",
      "python openmmrun.py -r --report-interval 1 -p CPU --store-interval 1 -t worker://initial.pdb --length 100 worker://traj/\n",
      "Move('worker://traj/' > 'sandbox:///{}/00000076/)\n",
      "echo \"This new line is pointless\"\n",
      "<posttask>\n"
     ]
    }
   ],
   "source": [
    "print task.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected this line was added to the end of the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add staging actions\n",
    "\n",
    "To set staging is more difficult. The reason is, that you normally have no idea where files are located and hence writing a copy or move is impossible. This is why the staging commands are not bash lines but objects that hold information about the actual file transaction to be done. There are some task methods that help you move files but also files itself can generate this commands for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move one trajectory (directory) around a little more as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traj = project.trajectories.one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy('sandbox:///{}/00000010/' > 'worker://)\n"
     ]
    }
   ],
   "source": [
    "transaction = traj.copy()\n",
    "print transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like in the script. The default for a copy is to move a file or folder to the worker directory under the same name, but you can give it another name/location if you use that as an argument. Note that since trajectories are a directory you need to give a directory name (which end in a `/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy('sandbox:///{}/00000010/' > 'worker://new_traj/)\n"
     ]
    }
   ],
   "source": [
    "transaction = traj.copy('new_traj/')\n",
    "print transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to move it not to the worker directory you have to specify the location and you can do so with the prefixes (`shared://`, `sandbox://`, `staging://` as explained in the previous examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy('sandbox:///{}/00000010/' > 'staging:///cached_trajs/)\n"
     ]
    }
   ],
   "source": [
    "transaction = traj.copy('staging:///cached_trajs/')\n",
    "print transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `.copy` you can also `.move` or `.link` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy('file://{}/alanine.pdb' > 'staging:///delete.pdb)\n",
      "Move('file://{}/alanine.pdb' > 'staging:///delete.pdb)\n",
      "Link('file://{}/alanine.pdb' > 'staging:///delete.pdb)\n"
     ]
    }
   ],
   "source": [
    "transaction = pdb_file.copy('staging:///delete.pdb')\n",
    "print transaction\n",
    "transaction = pdb_file.move('staging:///delete.pdb')\n",
    "print transaction\n",
    "transaction = pdb_file.link('staging:///delete.pdb')\n",
    "print transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mention these because they require special treatment. We cannot (like RP can) copy files to the HPC, we need to store them in the DB first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_pdb = File('file://../files/ntl9/ntl9.pdb').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you use `file://` to indicate that you are using a local file. The above example uses a relative path which will be replaced by an absolute one, otherwise we ran into trouble once we open the project at a different directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/jan-hendrikprinz/Studium/git/adaptivemd/examples/files/ntl9/ntl9.pdb\n"
     ]
    }
   ],
   "source": [
    "print new_pdb.location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now there are 3 `/` in the filename, two from the `://` and one from the root directory of your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load()` at the end really loads the file and when you save this `File` now it will contain the content of the file. You can access this content as seen in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRYST1   50.000   50.000   50.000  90.00  90.00  90.00 P 1            \n",
      "ATOM      1  N   MET     1      33.720  28.790  34.120  0.00  0.00           N  \n",
      "ATOM      2  H1  MET     1      33.620  29.790  33.900  0.00  0.00           H  \n",
      "ATOM      3  H2  MET     1      33.770  28.750  35.120  0.00  0.00 \n"
     ]
    }
   ],
   "source": [
    "print new_pdb.get_file()[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For local files you normally use `.transfer`, but `copy`, `move` or `link` work as well. Still, there is no difference since the file only exists in the DB now and copying from the DB to a place on the HPC results in a simple file creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to add a command to the staging and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer('file://{}/ntl9.pdb' > 'worker://ntl9.pdb)\n"
     ]
    }
   ],
   "source": [
    "transaction = new_pdb.transfer()\n",
    "print transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task.append(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: TrajectoryGenerationTask(OpenMMEngine) [created]\n",
      "\n",
      "Sources\n",
      "- staging:///integrator.xml \n",
      "- staging:///alanine.pdb \n",
      "- staging:///openmmrun.py \n",
      "- file://{}/ntl9.pdb [exists]\n",
      "- staging:///system.xml \n",
      "Targets\n",
      "- sandbox:///{}/00000076/\n",
      "Modified\n",
      "\n",
      "<pretask>\n",
      "Link('staging:///alanine.pdb' > 'worker://initial.pdb)\n",
      "Link('staging:///system.xml' > 'worker://system.xml)\n",
      "Link('staging:///integrator.xml' > 'worker://integrator.xml)\n",
      "Link('staging:///openmmrun.py' > 'worker://openmmrun.py)\n",
      "Touch('worker://traj/')\n",
      "python openmmrun.py -r --report-interval 1 -p CPU --store-interval 1 -t worker://initial.pdb --length 100 worker://traj/\n",
      "Move('worker://traj/' > 'sandbox:///{}/00000076/)\n",
      "echo \"This new line is pointless\"\n",
      "Transfer('file://{}/ntl9.pdb' > 'worker://ntl9.pdb)\n",
      "<posttask>\n"
     ]
    }
   ],
   "source": [
    "print task.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have one more transfer command. But something else has changed. There is one more files listed as required. So, the task can only run, if that file exists, but since we loaded it into the DB, it exists (for us). For example the newly created trajectory `25.dcd` does not exist yet. Would that be a requirement the task would fail. But let's check that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pdb.exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have now the PDB file staged and so any real bash commands could work with a file `ntl9.pdb`. Alright, so let's output its stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task.append('stat ntl9.pdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that usually you place these stage commands at the top or your script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could run this task, as before and see, if it works. (Make sure you still have a worker running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project.queue(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check, that the task is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'success'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did not screw up the task, it should have succeeded and we can look at the STDOUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:11:19 [worker:3] stdout from running task\n",
      "GO...\n",
      "Reading PDB\n",
      "Done\n",
      "Initialize Simulation\n",
      "Done.\n",
      "('# platform used:', 'CPU')\n",
      "('# temperature:', Quantity(value=300.0, unit=kelvin))\n",
      "START SIMULATION\n",
      "DONE\n",
      "Written to directory `traj/`\n",
      "This new line is pointless\n",
      "16777220 97338745 -rw-r--r-- 1 jan-hendrikprinz staff 0 1142279 \"Mar 21 13:11:18 2017\" \"Mar 21 13:11:15 2017\" \"Mar 21 13:11:15 2017\" \"Mar 21 13:11:15 2017\" 4096 2232 0 ntl9.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print task.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, great, we have the pointless output and the stats of the newly staged file `ntl9.pdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a real script look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun let's create the same scheduler that the `adaptivemdworker` uses, but from inside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from adaptivemd import WorkerScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = WorkerScheduler(project.resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really wanted to use the worker you need to initialize it and it will create directories and stage files for the generators, etc. For that you need to call `sc.enter(project)`, but since we only want it to parse our tasks, we only set the project without invoking initialization. You should normally not do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.project = project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a function `.task_to_script` that will parse a task into a bash script. So this is really what would be run on your machine now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\n",
      "# This is part of the adaptivemd tutorial\n",
      "ln -s ../staging_area/alanine.pdb initial.pdb\n",
      "ln -s ../staging_area/system.xml system.xml\n",
      "ln -s ../staging_area/integrator.xml integrator.xml\n",
      "ln -s ../staging_area/openmmrun.py openmmrun.py\n",
      "mkdir -p traj/\n",
      "python openmmrun.py -r --report-interval 1 -p CPU --store-interval 1 -t initial.pdb --length 100 traj/\n",
      "mkdir -p ../../projects/tutorial/trajs/00000076/\n",
      "mv traj/* ../../projects/tutorial/trajs/00000076/\n",
      "rm -r traj/\n",
      "echo \"This new line is pointless\"\n",
      "# write file `ntl9.pdb` from DB\n",
      "stat ntl9.pdb\n"
     ]
    }
   ],
   "source": [
    "print '\\n'.join(sc.task_to_script(task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you see that all file paths have been properly interpreted to work. See that there is a comment about a temporary file from the DB that is then renamed. This is a little trick to be compatible with RPs way of handling files. (TODO: We might change this to just write to the target file. Need to check if that is still consistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on file locations\n",
    "\n",
    "One problem with bash scripts is that when you create the tasks you have no concept on where the files actually are located. To get around this the created bash script will be scanned for paths, that contain prefixed like we are used to and are interpreted in the context of the worker / scheduler. The worker is the only instance to know all that is necessary so this is the place to fix that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that in a little example, where we create an empty file in the staging area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task = Task()\n",
    "task.append('touch staging:///my_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\n",
      "# This is part of the adaptivemd tutorial\n",
      "touch ../staging_area/my_file.txt\n"
     ]
    }
   ],
   "source": [
    "print '\\n'.join(sc.task_to_script(task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, the path has changed to a relative path from the working directory of the worker. Note that you see here the line we added in the very beginning of example 1 to our resource!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Task from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to start a new task you can begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = Task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just start adding staging and bash commands and you are done. When you create a task you can assign it a generator, then the system will assume that this task was generated by that generator, so don't do it for you custom tasks, unless you generated them in a generator. Setting this allows you to tell a worker only to run tasks of certain types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Python RPC Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks so far a very powerful, but they lack the possibility to call a python function. Since we are using python here, it would be great to really pretend to call a python function from here and not taking the detour of writing a python bash executable with arguments, etc... An example for this is the PyEmma generator which uses this capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an example of this as well. Assume we have a python function in a file (you need to have your code in a file so far so that we can copy the file to the HPC if necessary). Let's create the `.py` file now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_rpc_function.py\n"
     ]
    }
   ],
   "source": [
    "%%file my_rpc_function.py\n",
    "\n",
    "def my_func(f):\n",
    "    import os\n",
    "    print f\n",
    "    return os.path.getsize(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a PythonTask instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = PythonTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the call function has changed. Note that also now you can still add all the bash and stage commands as before. A PythonTask is also a subclass of `PrePostTask` so we have a `.pre` and `.post` phase available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_rpc_function import my_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the function `my_func` with one argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task.call(my_func, f=project.trajectories.one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: PythonTask(NoneType) [created]\n",
      "\n",
      "Sources\n",
      "- staging:///_run_.py \n",
      "- file://{}/_rpc_input_0x71bdd2d10e2f11e7a0f00000000002eaL.json \n",
      "- file://{}/my_rpc_function.py [exists]\n",
      "Targets\n",
      "- file://{}/_rpc_output_0x71bdd2d10e2f11e7a0f00000000002eaL.json\n",
      "Modified\n",
      "\n",
      "<pretask>\n",
      "Transfer('file://{}/_rpc_input_0x71bdd2d10e2f11e7a0f00000000002eaL.json' > 'worker://input.json)\n",
      "Link('staging:///_run_.py' > 'worker://_run_.py)\n",
      "Transfer('file://{}/my_rpc_function.py' > 'worker://my_rpc_function.py)\n",
      "python _run_.py\n",
      "Transfer('worker://output.json' > 'file://{}/_rpc_output_0x71bdd2d10e2f11e7a0f00000000002eaL.json)\n",
      "<posttask>\n"
     ]
    }
   ],
   "source": [
    "print task.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, interesting. What this actually does is to write the input arguments to the function into a temporary `.json` file on the worker, (in RP on the local machine and then transfers it to remote), rename it to `input.json` and read it in the `_run_.py`. This is still a little clumsy, but needs to be this way to be RP compatible which only works with files! Look at the actual script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see, that we really copy the `.py` file that contains the source code to the worker directory. All that is done automatically. A little caution on this. You can either write a function in a single file or use any installed package, but in this case the same package needs to be installed on the remote machine as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project.queue(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And wait until the task is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project.wait_until(task.is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default settings will automatically save the content from the resulting output.json in the DB an you can access the data that was returned from the task at `.output`. In our example the result was just the size of a the file in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can use this information in an adaptive script to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### success callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we did not talk about is the possibility to also call a function with the returned data automatically on successful execution. Since this function is executed on the worker we (so far) only support function calls with the following restrictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. you can call a function of the related generator class. for this you need to create the task using `PythonTask(generator)`\n",
    "2. the function name you want to call is stored in `task.then_func_name`. So you can write a generator class with several possible outcomes and chose the function for each task.\n",
    "3. The `Generator` needs to be part of `adaptivemd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the case of `modeller.task_run_msm_files` we create a `PythonTask` that references the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task = modeller.task_run_msm_files(project.trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'then_func'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.then_func_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will call the default `then_func` of modeller or the class modeller is of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function then_func in module adaptivemd.analysis.pyemma.emma:\n",
      "\n",
      "then_func(project, task, model, inputs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(modeller.then_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These callbacks are called with the current project, the resulting data (which is in the modeller case a `Model` object) and array of initial inputs.\n",
    "\n",
    "This is the actual code of the callback\n",
    "\n",
    "```py\n",
    "@staticmethod\n",
    "def then_func(project, task, model, inputs):\n",
    "    # add the input arguments for later reference\n",
    "    model.data['input']['trajectories'] = inputs['kwargs']['files']\n",
    "    model.data['input']['pdb'] = inputs['kwargs']['topfile']\n",
    "    project.models.add(model)\n",
    "```\n",
    "\n",
    "All it does is to add some of the input parameters to the model for later reference and then store the model in the project. You are free to define all sorts of actions here, even queue new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will talk about the factories for `Task` objects, called `generators`. There we will actually write a new class that does some stuff with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
